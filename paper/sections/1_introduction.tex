\begin{abstract}

Our analysis of the key-value activity generated by the ParSplice molecular
dynamics simulation demonstrates the need for more complex cache management
strategies. Baseline measurements show clear key access patterns and hot
spots that offer significant opportunity for optimization. We use the data
management and policy engine from the Mantle system to dynamically explore a
variety of techniques, ranging from basic algorithms and heuristics to
statistical models, calculus, and machine learning. While Mantle was originally
designed for distributed file systems, we show how the collection of
abstractions effectively decomposes the problem into manageable policies for a
different domain and service.  Our exploration of this space results in a
dynamically sized cache policy that, for our initial conditions, sacrifices
negligible performance while using only 28\% of the memory required by our
hand-tuned cache.

\end{abstract}

\section{Introduction}

Storage systems use software-based caches extensively to improve performance
but the policies that guide what data to evict and when to evict vary with the
use case. For example, caching file system metadata on clients and servers
reduces the number of remote procedure calls and improves the performance of
create-heavy workloads common in HPC~\cite{ren:sc2014-indexfs,
patil:fast2011-giga+}. But these the policies for when to evict and what data
to evict are specific to the application's behavior and the system's
configuration (hardware, settings, etc.) so a new use case may prove to be a
poor match for the selected caching
policy~\cite{xiao:socc15-shardfs,brandt:msst2003-lh,sevilla:sc15-mantle,
weil:sc2004-dyn-metadata, weil:osdi2006-ceph}. In this paper, we evaluate a
variety of caching policies using a data management language/policy engine and
arrive at a customized policy that works well for our target application.  This
process of trying general policies and quickly iterating to a customized
solution shows that our prototype can adapt to different workloads.

%very customized solution confirms that our prototype can adapt to different
%types of workloads; but more importantly, this is the second system and domain
%that we have successfully used this policy language and engine in. This breadth
%helps us reason about more general data management policies by giving us a
%framework to compare and contrast policies from other domains and services.

Our target application, ParSplice~\cite{perez:jctc20150parsplice}, is
representative of these types of applications. ParSplice is a molecular
dynamics simulation that uses a hierarchy of caches and a single persistent
key-value store to store both observed minima across a molecule's equation of
motion (EOM) and the hundreds or thousands of partial trajectories calculated
each second during a parallel job.  The fine-grained data annotation
capabilities provided by key-value storage is a natural match for scientific
simulations like ParSplice. But these simulations rely on a mesh-based
decomposition of a physical region and result in millions or billions of mesh
cells, where each cell contains materials, pressures, temperatures and other
characteristics that are required to accurately simulate phenomena of interest.
Unfortunately, simulations of this size saturate the capacity and bandwidth
capabilities of a single node so we need more effective data management
techniques, such as cache management or load balancing across a cluster.

\begin{figure}[t]
\noindent\includegraphics[width=0.5\textwidth]{figures/cache-management.png}\\
\caption{Using our data management language and policy engine, we designed a
dynamically sized caching policy (thick line) for ParSplice.  Compared to
existing configurations (thin lines with \(\times\)'s), our solution saves the most
memory without sacrificing performance and works for a variety of inputs.
\label{fig:cache-management}}
\end{figure}

The biggest challenge for ParSplice is properly sizing the caches in the
storage hierarchy.  The memory usage for a single cache that stores molecule
coordinates is shown in Figure~\ref{fig:cache-management}, where the thin solid
lines marked with \(\times\)'s are the existing configurations in ParSplice.
The default configuration uses an unlimited sized cache, shown by the ``No
Cache Management" line, but using this much memory for one cache is
unacceptable for HPC environments, where a common goal is to keep memory for
such data structures below 3\%\footnote{Anecdotally, we find this threshold
works well for HPC applications.  For reference, a 1GB cache for a distributed
file system is too large in LANL deployments.}. Since ParSplice deploys a cache
per 300 worker processes, large simulations will not scale.  Users can
configure ParSplice to evict data when the cache reaches a threshold but this
solution requires tuning and parameter sweeps; the ``Cache (too small)" curve
in Figure~\ref{fig:cache-management} shows how a poorly configured cache can
save memory but at the expense of performance, which is shown by the
annotation.  Even worse, this threshold changes with different initial
configurations and clusters so tuning needs to be done for all system
permutations.  Our dynamically sized cache, shown by the think line in
Figure~\ref{fig:cache-management}, detects key access patterns and re-sizes the
cache accordingly.  Without tuning or parameter sweeps, our solution saves more
memory than a hand-tuned cache with a negligible performance degradation
(within the baseline's standard deviation) and works for a variety of initial
conditions.

% What is Mantle
To design more flexible cache management policies, like our solution in
Figure~\ref{fig:cache-management}, we use the data management language and
policy engine from the Mantle paper~\cite{sevilla:sc15-mantle}.  While Mantle
was designed for the narrow purpose of filesystem metadata load balancing, this
paper provides evidence that the approach is more broad.  Specifically, the
collection of abstractions in Mantle provides a general control plane that
improves the performance of metadata access. So in this paper we refer to
Mantle as a policy engine that injects policies written in our data management
language directly into a running service, such as a file system or key-value
store.  Rather than co-designing a policy directly into the service, which
requires domain-specific knowledge to find and change policies, we expose the
policies in a general way so even developers unfamiliar with the domain can
deploy quickly deploy solutions.  We show that our framework:

\begin{itemize}

  \item decomposes cache management into independent policies that can be
  dynamically changed, making the problem more manageable and easier to reason
  about.

  \item can deploy a variety of cache management strategies ranging from basic
  algorithms and heuristics to statistical models and machine learning.

  \item has useful primitives that, while designed for file system metadata
  load balancing, turn out to also be effective for cache management. 

\end{itemize}

% this gives us many policies that are effective across disciplines
% - reuse: eases burden of writing policies
% - autonomic: lays groundwork for an adaptable policy that mixes/matches policies
% FUTURE WORK

This last contribution is explored in Sections~\S\ref{sec:arch-specific}
and~\S\ref{sec:dom-specific}, where we try a range of policies from different
disciplines; but more importantly, in Section~\S\ref{sec:scope}, we conclude
that the collection of policies we designed for ParSplice's cache management
are very similar to the policies used to load balance metadata in the Ceph file
system (CephFS) suggesting that there is potential for automatically adapting
and generating policies dynamically.  

%Manageable: abstracts away complexities of the system (pass around to others,
%use different strategies) 

