\documentclass{article}
\usepackage{graphicx,color,setspace,hyperref}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\centcmark}{\centering\checkmark}

\lstset{language=C}


\begin{document}

\title{ Storage Bibliography }

\maketitle

%NOAH NODB: instant gateway to data
%- get rid of the load time (either before or within queries)
%1. selective tokenizing: doesn't need to parse more than column 4
%2. selective parsing: don't need column 4, don't convert
%3. selective tuple formation: don't allocate if you don't need it
%4. grow index over where the data is in the file (into byte stream)
%5. caching data that I parsed
%
%Dimitris: channels
%how do we use flash in a cloud distributed
%- use flash and hods
%
%Ivo: AscotDB
%- byte wrappers around scidb
%- change the logical coordinates of scientific data
%
%Carlos: Making the Web Faster with HTTP 2.0
%- extensibility in storage by thinking of RESTful architectures
%- put a stream layer between http and text
%- fix multiplexing everything in a  TCP connection with streams
%- content headers (1) streaming (2) multiplexing in a  stream
%
%Adam:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File Systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{File Systems}

\noindent\cite{mckusick:acm1984-FFS} FFS - establishes variable sized blocks for optimal storage, layout awareness to exploit locality of r eference, and a new way to organize data.\\

\noindent\cite{mckusick:fast2015-FFS} History of BSD Filesystem - decoupling policy from mechanism in the block allocator. \\

\noindent\cite{hitz:wtec1994-WAFL} WAFL - focuses on write requests (buffered, parallel, anywhere) instead of read requests so that we can change the layout the data and snapshots.\\

\noindent\cite{lanyue:fast2013-FS-evolution} FS evolution - studies where most of the bugs come from, what they mean, where most of the development time is spent, and what affects performance; useful for the pointers to these FSs.\\

\noindent\cite{rosenblum:acm1992-LFS} LFS - writes will be bottleneck and we have larger caches/faster CPUs; (1) buffer file changes, (2), i-node map, (3) segment cleaning.\\

\noindent\cite{rumble:fast2014-lfs-for-dram} LFS for RAMCloud - storage workloads incur 2X the memory space, so use LFS techniques for memory allocation - 2 level incremental cleaning solution with concurrent, minimal memory copy, no global cleaning.\\

\noindent\cite{aghayev:inflow2013-lfs-for-android} LFS for Android - HTTP browser cache incurs many random writes, which perform poorly on flash; they trade hit rate for performance. Implementation: allocate a large file, divide into segments, evict many entries \(\in\) segment; avoids segment cleaning because there is no copying involved.\\

\noindent\cite{kang:fast2014-multilanes} MultiLanes - ext4, xfs, btrfs are poor  file systems for the host/hypervisor when housing VMs, so deliver IO requests to driver directly.\\

\noindent\cite{he:fast2015-chopper} Chopper - discovers high latency operations using Latin Hypercube Sampling. They expose problematic block allocation algorithms in ext4 for corner cases and fix them. \\

\noindent\cite{wildani:systor2011-block-locality} Working sets in block IO streams - finds temporal and spatial locality for blocks by (1) calculating offset differences biased with time and (2) partitioning the accesses into distinct sets; but this is for data and does not reflext what the application is doing. \\

\noindent\cite{nightingale:sosp2005-speculator} Speculator - overlaps computation by executing with stale data; when the asynchronous reply returns, computation may be rolled back if the data is invalid.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RAID
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RAID}

\noindent\cite{stodolsky:techreport1994-redundant} RAID Overview - shows that different RAID levels utilize a variety of striping, mirroring, and parity techniques to store data. \\

\noindent\cite{stodolsky:comparch1993-paritylogging} Small Writes - describes the small-write problem - four accesses are needed for each small-write instead of two.\\

\noindent\cite{wacha:venue2010-RAID4s} RAID4S - adds an SSD as the parity drive in RAID4 schemes in order to increase the throughput of small writes.\\

\noindent\cite{mao:ipsps2010-HPDA} HPDA - proposes the HPDA solution, which uses SSDs as the data disks and two hard disks drives for the parity drive and a RAID1-style write buffer.\\

\noindent\cite{hitz:wtec1994-WAFL} WAFL - to get more space/performance via the network, they change  writes: (1) buffered, (2) performed in parallel, and (3) placed anywhere. This work shows that writes are slower than reads, that disk layout matters (tree of files, metadata + file), and that snapshots cans be fast/efficient.\\

\noindent\cite{im:msst2010-delayedparity} Delayed Parity - proposes a delayed parity techniques to log the parity updates, instead of the small-writes, on a partial parity cache.\\
f
\noindent\cite{park:cit2009-ssdRAID} Hetero SSDs - presents heterogenous SSD based RAID4, which injects SSDs into a RAID4 setup by replacing all the data drives with SSDs.\\

\noindent\cite{chen:acm1994-RAID} Parity Overview - shows that RAID schemes based on parity enhance storage reliability by managing a recovery parity disk, but parity calculations degrade performance.\\

\noindent\cite{XDD} XDD - users guide.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Distributed Storage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributed Object Stores}
\noindent\cite{weil:osdi2006-ceph} Ceph - distributed file system decouples metadata/data, dynamically distributes metadata, and leverages OSDs so they can do most of the system maintenance on their own.\\

\noindent\cite{beaver:osdi2010-haystack} Haystack - custom object store that avoids metadata operations by keeping logical \(\mapsto\) physical mappings in a Directory and volume maps in memory on the Store devices. Leverages the fact that pictures/videos are large, immutable objects that don't ``seek" functionality.\\

\noindent\cite{muralidhar:osdi2014-f4} f4 - erasure coded storage system for warm data that requires less throughput. They start by demonstrating that temperature zones (unchanging content with a low request rate) do exist.  Pushing computation to the storage nodes not always a good idea, as sometimes you want them to just serve data.\\

\noindent\cite{toor:nas2012-swift} Swift - looked at the applicability of Swift for CERN experiments; non-filesystem work shows that scalability is of the data/metadata is good, when using a database.\\

\noindent\cite{thusoo:sigmod2010-facebook-infrastructure} Facebook - multiple compute silos; limitations in the namenode (\# of files, memory) have them looking to distribute data across many clusters. Workloads invoke many small files, which have inspired concatenation and compression. \\

\noindent\cite{nightingale:osdi2012-fds} FDS - locality-oblivious blog store that exposes a flat namespace.
\begin{itemize}
	\item blobs, which are broken into tracts (R/W granularity), are stored on disks managed by tractservers (i.e. OSD). Tracts are striped across the system.
	\item MDS stores/distributes maps of hardware 
	\item each client can do a map-based table lookup by hashing the blobID and track number \(\rightarrow\) one hop everywhere \(\rightarrow\) equal I/O bandwidth everywhere \(\rightarrow\) no need for data locality
	\item table has \(n^2\) rows to store every pair of disks \(\rightarrow\) reduce TLT size \(\rightarrow\) push work to OSD
	\item \(\rightarrow\) flat \(\rightarrow\) no namespace \(\rightarrow\)  hash objects \(\rightarrow\) fast lookups \(\rightarrow\)  metadata with data
	\item exposes full disk bandwidth to all processors \(\rightarrow\) (1) recovery is fast (total disk contents on \(n\) disks) (2) not restrict to data locality programming models, e.g., MapReduce so no stragglers because you can dynamically assign work, and (3) hardware heterogeneity because I/O and compute can be purchased/managed separately.
	\item evenly distribute replicas (disk has replicas on \(n\) disks) \(\rightarrow\) use bandwidth of entire cluster to recover
	\item recovery: all disks participate in recovery by striping blob tracts among all disks in the cluster.
	\item results: \(\frac{1}{2}\) bandwidth of local disk, peaks at cluster bandwidth
	\item[\xmark] no file system locality
	\item[\xmark] no stripe blobs
	\item[\xmark] why always comparing to GFS?
	\item[\xmark] every node has network bandwidth equal to disk bandwidth, they each have as much network bandwidth as they need I/O bandwidth		
\end{itemize}

\noindent\cite{mickens:ndsi2014-blizzard} Blizzard - cloud-scale block store (1200 MB/s) that uses FDS to avoid performance tiers. Reduces write barrier and supports small random IO with:
\begin{itemize}
	\item nested striping: fine-grain virtual disk partitioning; avoids IOP convoy dilation (i.e. increased inter-request latency due to network jitter) and increases disk parallelism. 
	\item asynchronous flushes: immediately acknowledged while data flushed in background - lengthens vulnerability period but allows higher write performance. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Distributed Key-Value Stores
\section{Distributed Key-Value Stores}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\cite{chang:osdi2006-bigtable} BigTable - distributed key-value store tailored for performance that accepts variable data and multiple data attributes (infinite columns). Similar rows are grouped into tablets (put on servers), similar columns are grouped into locality groups (put into memory), and everything is managed by a Master/Chubby.\\

\noindent\cite{decandia:sosp2007-dynamo} Dynamo - distributed ``eventually consistent" key-value store tailored for reliability/availability that uses consistent hashing and quorums (allow the programmer to control R, W, N replication).\\

\noindent\cite{} Megastore - ACID compliance using commit logs and PAXOS to agree on a timestamp for a transaction.\\

\noindent\cite{} Spanner - SQL query language, global consistency/transactions, better write performance, lock free read-only transactions.\\ 

\noindent\cite{} F1 - SQL semantics with noSQL scalability. Takes Spanner's global transactions, consistency, and transparent sharding and adds distributed SQL queries, transactionally consistent secondary indexes, asynchronous schema changes including database reorganizations, optimistic transactions, and automatic change history recording and publishing.\\

\noindent\cite{greenberg:hotstorage2015-mdhim} MDHIM - multi-D hierarchical
indexing middleware: key-value store designed for HPC and multi-dimensional
data. 

\begin{itemize}
  \item mechanisms: cursor types, bulk operations
  \item policies: partitioning, server allocation, secondary indices
  \item hpc tailored: infiniband, dynamic servers, pluggable DB, library
\end{itemize}

%Problem: cloud key-value stores don't perform well for tightly coupled and
%parallel supercomputers and large data sets; HPC has simulation data,
%structured (indexed in multiple dimensions), PB scale, infiniband/RDMA,
%resource manager (no deamones)
%
%Existing: cloud kv-stores are diverse/unstructured, usually partitioned
%w/consistent hash
%
%Motivation: PLFS can materialize reads on compute cluster; leverage big data
%capabilities within an HPC environment, e.g., coordinates = key enhances
%locality, access rate, semantically meaningfulG
%
%Solution: MDHIM 
%  - HPC: infiniband, dynamic servers, pluggable DB, leverage resources, library
%  - partitioned by range (not hash): # of spawned servers affects performance
%  - cursor type operations and bulk ops: off-load query decision making to clients
%  - multiple dimensions: users create secondary index instances
%
%Why does PLFS need a file system back end in the first place?  How does ingest
%work if they are spawned at job start?



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metadata Management
\section{Metadata Management}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent namespace = hierarchy, record changes to files or properties\\

\subsection{Single MDS}
\noindent\cite{ghemawat:sosp2003-gfs} GFS - single node authority (Master) manages metadata
\begin{itemize}
	\item metadata = namespace, ACLs, files \(\mapsto\) chunks, chunk \(\mapsto\) chunk server
	\item management = chunk lease, garbage collection, chunk migration
	\item facilities many failures, huge files, and sequential access using a metadata server and chunk servers for housing large byte sequences, which reduce metadata/network overhead and reliance on master); first to separate data/metadata I/O.
\end{itemize}

\noindent\cite{mckusick:acm2010-gfs-evolution} GFS metadata server is overwhelmed.\\

\noindent\cite{shvachko:login2012-hdfs-scalability} HDFS - single node authority (Namenode) manages metadata
\begin{itemize}
	\item metadata = namespace, access control, files \(\mapsto\) blocks, block \(\mapsto\) data node
	\item management = replication based on heartbeats/blockreports, rebalancing not implemented
	\item scalability - good overview and explains why the system is not scalable (bad for writes because of journaling). Namenode holds file system index and file system name node.
\end{itemize}

\subsection{Hashing/Table-based Mapping}
\noindent\cite{schmuck:fast2002-gpfs} GPFS - hash filename to find disk, metanode updates metadata
\begin{itemize}
	\item metadata = files \(\mapsto\) blocks, size, ACLs
	\item token manager = determines metanode (synchronize access to metadata)
	\item distributed locking for data, synchronization for metadata; clients only write to parts of a file
\end{itemize}

\noindent\cite{brandt:mss2003-lh} Lazy Hybrid - hashes pathname to locate metadata but maintains permissions and file system nameespace using tables (lookup MDS, access control), invalidation (broadcast renames/removes), lazy update/migration (add servers, delete files), and consistency semantics. \\

\noindent\cite{hildebrand:msst2005-pnfs} PVFSv2/pNFS - hash filename to find server, which holds a Berkeley DB
\begin{itemize}
	\item NFSv4 binds one network endpoint to all files
	\item allows parallel access by passing layout from storage system to clients
\end{itemize}

\noindent\cite{li:msst2006-dynamic} Dynamic Hashing - adds a metadata cache, directory hashing, and dynamic formulas. Formulas account for a forgetting factor, access information, and the number of MDSs leaving to account for load balancing and elastic clusters.\\

\noindent\cite{xing:sc2009-skyfs} skyFS - hashes filename to find server
\begin{itemize}
	\item (1) directory fragmentation for large directories, (2) parallelize within directories, (3) cache metadata to improve locality, (4) consistent hashing minimizes the amount of data that needs to be moved when servers are added/deleted (servers contact masters if overloaded, migrate tree partition with a hash)
\end{itemize}

\noindent\cite{zhu:pds2008-hba} Hierarchical Bloom Filter Arrays - distributed randomly, lookup location in replicated table
\begin{itemize}
	\item metadata: namespace, access control, files \(\mapsto\) blocks, block \(\mapsto\) node
	\item management: synchronize metadata tables
	\item insight: temporal locality is small
	\item 1st level gives accurate hot data, 2nd level gives inaccurate all data
\end{itemize}



\noindent\cite{sinnamohideen:atc2010-ursa} Ursa Minor - single node authority (root metadata server) manages metadata location; metadata distributed by enumerating according to namespace; multiple servers manage metadata requests
\begin{itemize}
	\item metadata = size, link count, attributes, permissions, files \(\mapsto\) nodes
	\item metadata servers access metadata objects in lower storage system; namespace servers manage directories
	\item good locality for multi-object operations, transactions
\end{itemize}

\noindent\cite{patil:pdsw2007-giga+} GIGA+ - incremental growth and minimal shared state is motivation for 2-level metadata and inconsistent metadata.\\

\noindent\cite{patil:fast2011-giga+} GIGA+ - splits directories when the entries reach a threshold by exercising lazy synchronization and redirecting queries to servers with histories of the partition splits.\\

\noindent\cite{ren:atc2013-tablefs} TableFS - stores small metadata (directories, inodes) and files in a LSM database so that they are written to the local FS as large objects (write-ahead logs, SSTables, large files). \\

\noindent\cite{ren:sc2014-indexfs} IndexFS - hashes directories, moves whole directories to different nodes, clients cache paths and permissions, logs stored in LSM for fast insertion and lookup. 

	\begin{enumerate}
		\item dyn.nspace partition: migrate dirfrags (round-robin)
		\item stateless dir caching: MDSs pin pathnames/permissions
		\begin{itemize}
			\item clients cache server names, block until lease expires
		\end{itemize}
		\item LSM tree for metadata \(\rightarrow\) sort, index, write-ahead log
		\begin{itemize}
			\item TableFS: embed inodes/small files, column-style table 
		\end{itemize}
		\item Metadata bulk insert: clients cache all creates, lock dir
	\end{enumerate}
	CephFS Advantage
	\begin{itemize}
		\item production-level file system
		\item integrated into RADOS and built from the ground up
		\item monitoring or reporting system
		\item more of a general purpose file system 
	\end{itemize}
	
\noindent\cite{zheng:pdsw2014-batchfs} BatchFS (non-POSIX) - push computation to the clients (``lazy namespace synchronization": local operations merge with global namespace image) to avoid MDS synchronization problems ({\it e.g.,} directory lock contention, serialized transactions, and RPCs).\\

\noindent\cite{thomson:fast2015-calvinfs} CalvinFS - hashes filename to locate server with metadata. Handles many small files and fully linearizable random writes using the feature rich Calvin database, which has support for WAN/LAN replication, OLLP for mid-commit commits, and a sophisticated logging subsystem. The authors opt for balanced metadata distribution by hashing the filename to identify the metadata server, but admit to losing locality for creating files, deleting files, and changing permissions.
\subsection{Subtree Partitioning}

\noindent\cite{welch:fast2008-panasas} Panasas - each MDS manages a subtree bounded by the volume that it is introduced in. Each directory and its files within must be managed by the same MDS, which allows separate fault domains (i.e. no random failures and outages). There is no directory fragmenting or dynamic balancing.\\

\noindent\cite{weil:sc2004-dyn-metadata} Dynamic Subtree Partitioning (DynSP) - describes current approaches to managing metadata (subtree partitioning/hashing) and motivates the need for dynamic metadata (authority/collaborative caching, hierarchies, load balancing, traffic control, directory locality, storage). \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programmable Storage Systems
\section{Programmable Storage Systems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s%%

\noindent\cite{grawinkel:pdsw2012-lua} Scripted pNFS Layouts - application matches file type to layout algorithm, such as placement/striping, using Lua.\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metadata Studies
\section{Metadata Studies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s%%
\noindent\cite{roselli:atec2000-FS-workloads} FS Workloads - as much as 50\% of all workloads is composed of metadata operations (this is important). \\

\noindent\cite{alam:pdsw2011-metadata-scaling} Metadata Wall - theoretical ops/second are not realized for the metadata on Lustre/GPFS; bottlenecks are the simultaneous creates and fast creates/deletes, so performance cannot be improved by deploying a large volume of hardware.\\

\noindent\cite{konstantinos:pdsw2014-lustre-metadata} Lustre's MDS - performance decreases with more sockets because of the thread limitations in the Linux kernel. Also, the back end is not a bottleneck.\\

\noindent\cite{abad:techreport2012-fstrace, abad:ucc2012-mimesis} Mimesis - limited metadata traces and workload models motivates work in statically modeling workloads; useful for the limitations of metadata benchmarks. \\

\noindent\cite{leung:atc2008-nfs-trace} - surveys traces from papers; discovers that traces are old and not repressentative of true metadata workloads. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Accessing Metadata
\section{Accessing Metadata}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\cite{hua:sc2009-smartstore} SmartStore - applies correlation to metadata to make search fast and responsive. \\

\noindent\cite{leung:fast2009-spyglass} Spyglass - makes file system metadata search faster by introducing a smart crawler that extracts metadata and an index that stores/serves metadata.\\

\noindent\cite{cipar:eurosys2012-lazybase} LazyBase - pipelines metadata updates through database transformations allowing arbitrary injection to satisfy different amounts of freshness and performance.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dynamic Load Balancing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dynamic Load Balancing}

\noindent\cite{anderson:fast2001-hippodrome} Hippodrome - iterative design tool that tries to provision enough storage nodes to achieve optimal throughput, balance, and utilization by analyzing the workload, modeling the performance, finding a solution, and migrating data. They show that the solution converges and allocates a reasonable amount of resources.\\

\noindent\cite{brandt:rtss2003-rad} RAD/RBED - real-time scheduling model that separates resource allocation from timing of delivery so that we can service different real-time and non-real-time-scheduling classes; RBED is a scheduler that assigns a target rate of progress and period to each process. Rates can dynamically adjusts for processes not making enough progress or processes coming into and out of the system. \\

\noindent\cite{povzer:eurosys2008-fahrrad} Fahrrad - use reservations for utilization and period (based off of RAD/RBED with the limitation that I/O cannot be preempted); achieves better hard guarantees while maintaining performance best-efforts I/O schedulers. \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Request-Type Locality
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dynamic Load Balancing}
\noindent\cite{pai:asplos1998-lard} LARD - algorithm for a proxy to distribute requests that emphasizes locality by sending requests to back-end nodes based on their content and the node's utilization (connections). \\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fault Tolerance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fault Tolerance}

\noindent\cite{sundararaman:fast2010-membrane} Membrane - on file system crash, resume execution via logging and snapshots. \\

\noindent\cite{swift:sosp2003-nook, swift:osdi2004-shadow-drivers} memory management hardware - encapsulate DD in protected domain (a.k.a ``hard-wall"). When faults occur, enable address-space isolation around the driver~\cite{swift:sosp2003-nook} and microreboot the DD~\cite{swift:osdi2004-shadow-drivers} to stop errant code from writing to the kernel. \\

\noindent\cite{zhou:osdi2006-safedrive} SafeDrive - compile assertion code into DD that checks segfaults and restarts driver.\\

\noindent\cite{david:osdi2008-curios} microkernel techniques - create a new protection domain to house session state.\\

\noindent\cite{lu:osdi2014-icefs} IceFS - disentangles file system into cubes to provide localized reaction to faults, fast recovery, and concurrent updates. Logical entities physically separated to provide localization, specialization, and isolation. Achieved with no shared physical resources ({\it e.g.}, blocks), access dependencies ({\it e.g.}, metadata path traversal) and bundled transactions ({\it e.g.}, group updates into a global transaction). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Archival
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Archival}
\noindent\cite{adams:techreport2012-archival} Scientific/Historical Repositories - Showed that tertiary data is stored on disk, is highly dynamic, and subject to crawls.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programmable Storage Systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programmable Storage Systems}

\noindent\cite{spahn:osdi2014-pebbles} Pebbles: data management tools built on the notion of objects (both at the storage system and application level). \\
Problem: modern OSs lack the appropriate abstractions to manage data (only acts at file level; as a result, applications must manage data. Applications don't do well managing data because objects are mapped to files in the OS, which is application-specific, complex, and too coarse-grained. As a result, many applications don't do what they say the will (e.g., delete an object completely). It would be nice if we could teach the OS how to manage data.
\begin{enumerate}
	\item Pebbles Android: modified SQLite, XML, FS to use TaintDroid to track data flow through the system; feeds access data to...
	\item Pebbles Object Manager: counstruct uni-directional relationships based on access
\end{enumerate}
Advantages: users mark objects to audit in Pebbles application, then use regular application to delete/modify object
\begin{itemize}
	\item encryption/access/object manager improves flexibility/manageability of objects across software stacks
	\item no application user-level objects by leveraging the structure in databases
	\item automatically discovers objects by leveraging database constructs
\end{itemize}
Conclusion: finding these objects is possible and these abstractions can provide new functionality for objects.\\

\noindent\cite{seshadri:osdi2014-willow} Willow: expose APIs for both the application and the SSD-side CPUs.\\
Motivation: SSDs already have software-defined storage. This is analogous to GPUs; initially they had rigid interface, but inside there was hidden, programmable firmware. As they become more flexible, the began exposing partial programmability (shaders) and then full blown programmability (Tesla).
\begin{itemize}
	\item SSDs had a fixed interface (SATA/NVMe), but inside they have multi-core processors, firmware
	\item Goals: data dependent accesses, semantic extension, offload privilege execution
\end{itemize}
Architecture: custom code on the host and SPUs, designed as SSDApps by programmer
\begin{itemize}
	\item system installs/deploys code that (1) runs on SPU, (2) runs in Application, (3) runs in kernel
	\item system uses RPCs to communicate and do access control with process ID taint that flows through the kernel
\end{itemize}
Example application: write-ahead logging system based on an editable atomic write (log writes in SSD Willow application, metadata file points to log file and data file; data flushed on commit)
\begin{itemize}
	\item re-implemented Verilog application that uses a sophisticated data structure over direct-IO
	\item advantage: 50\% improvement in performance because we can simplify write-ahead logging scheme and push it to SSD
\end{itemize}
Advantage: improve performance with kernel-bypass, data dependent access, and SSD-side permission checks. Improve flexibility with semantic extension.\\

\noindent\cite{jenkins:ipdsw17-mochi} Mochi - memoization (computation caching) as a service used multiscale physics (CoEVP). 


	
\bibliographystyle{abbrv}
\bibliography{references}

\addcontentsline{toc}{section}{References} 


\newpage
\onecolumn
\appendix
%Appendix A
%\section{1}


\end{document}
